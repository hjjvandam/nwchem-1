A more complete description should be available at 
%\begin{verbatim}
%   http://emsl.pnl.gov:2080/docs/nwchem/nwchem.html
\htmladdnormallink{http://www.emsl.pnl.gov:2080/docs/nwchem/nwchem.html}
{http://www.emsl.pnl.gov:2080/docs/nwchem/nwchem.html}
%\end{verbatim}

The command required to invoke NWChem is machine dependent, whereas
most of the NWChem input is machine independent\footnote{Machine
dependence within the input arises from file names, machine
specific resources, and differing services provided by the operating system.} .

\section{Sequential execution}

To run NWChem sequentially on nearly all UNIX-based platforms simply
use the command \verb+nwchem+ and provide the name of the input file
as an argument (section \ref{sec:inputstructure}).

Output is to standard output, standard error and Fortran unit 6
(usually the same as standard output).  Files are created by default
in the current directory, though this may be overridden in the input
(section \ref{sec:dirs}).

\section{Parallel execution on UNIX-based parallel machines
including workstation clusters using TCGMSG}
\label{sec:procgrp}

 These platforms require the use of the TCGMSG\footnote{Where required
TCGMSG is automatically built with NWChem.} \verb+parallel+ command
and thus also require the definition of a process-group (or procgroup)
file.  The process-group file describes how many processes to start,
what program to run, which machines to use, which directories to work
in, and under which userid to run the processes.  By convention the
process-group file has a \verb+.p+ suffix.

The process-group file is read to end-of-file.  The character \verb+#+
(hash or pound sign) is used to indicate a comment which continues to
the next new-line character.  Each line describes a cluster of
processes and consists of the following whitespace separated fields:

\begin{verbatim}
  userid hostname nslave executable workdir
\end{verbatim}

\begin{itemize}
\item \verb+userid+ -- The user-name on the machine that will be executing the
      process. 

\item \verb+hostname+ --  The hostname of the machine to execute this process.
             If it is the same machine on which parallel was invoked
             the name must match the value returned by the command 
             hostname. If a remote machine it must allow remote execution
             from this machine (see man pages for rlogin, rsh).

\item \verb+nslave+ --  The total number of copies of this process to be executing
             on the specified machine. Only ``clusters'' of identical processes
             specified in this fashion can use shared memory to communicate.
             If no shared memory is supported on machine \verb+<hostname>+ then
             only the value one (1) is valid.

\item \verb+executable+ --  Full path name on the host \verb+<hostname>+ of the image to execute.
             If \verb+<hostname>+ is the local machine then a local path will
             suffice.

\item \verb+workdir+ --  Full path name on the host \verb+<hostname>+ of the directory to
             work in. Processes execute a chdir() to this directory before
             returning from pbegin(). If specified as a ``.'' then remote
             processes will use the login directory on that machine and local
             processes (relative to where parallel was invoked) will use
             the current directory of parallel.
\end{itemize}

  For example, if your file \verb+"nwchem.p"+ contained the following
\begin{verbatim}
 d3g681 pc 4 /msrc/apps/bin/nwchem /scr22/rjh
\end{verbatim}
then 4 processes running NWChem would be started on the machine 
\verb+pc+ running as user \verb+d3g681+ in directory \verb+"/scr22/rjh"+.
To actually run this simply type:
\begin{verbatim}
  parallel nwchem big_molecule.nw
\end{verbatim}

{\em N.B.} : The first process specified (process zero) is the only
process that
\begin{itemize}
\item opens and reads the input file, and
\item opens and reads/updates the database.
\end{itemize}
Thus, if your file systems are physically distributed (e.g., most
workstation clusters) you must ensure that process zero can correctly
resolve the paths for the input and database files.

{\em N.B.}  In releases of NWChem prior to 3.3 additional processes
had to be created on workstation clusters to support remote access to
shared memory.  This is no longer the case.  The TCGMSG process group
file now just needs to refer to processes running NWChem.

\section{Parallel execution on MPPs}

All of these machines require use of different commands in order to
gain exclusive access to computational resources.

\section{IBM SP}

If using POE (IBM's Parallel Operating Environment) interactively,
simply create the list nodes to use in the file \verb+"host.list"+ in
the current directory and invoke NWChem with
\begin{verbatim}
  nwchem <input_file> -procs <n>
\end{verbatim}
where \verb+n+ is the number of processes to use.  Process 0 will run
on the first node in \verb+"host.list"+ and must have access to the
input and other necessary files.  Very significant performance gains
may be had by setting the following environment variables before
running NWChem (or setting them using POE command line options).
\begin{itemize}
\item \verb+setenv MP_EUILIB us+ --- dedicated user space
  communication over the switch (the default is IP over the switch
  which is much slower).
\item \verb+setenv MP_CSS_INTERRUPT yes+ --- enable interrupts when a 
  message arrives (the default is to poll which significantly slows
  down global array accesses).
\end{itemize}
In addition, if the IBM is running PSSP version 3.1, or later
\begin{itemize}
\item \verb+setenv MP_MSG_API lapi+, or 
\item \verb+setenv MP_MSG_API mpi,lapi+ (if using both GA and MPI) 
\end{itemize}

For batch execution, we recommend use of the \verb+llnw+ command which
is installed in \verb+/usr/local/bin+ on the EMSL/PNNL IBM SP.
Interactive help may be obtained with the command \verb+llnw -help+.
Otherwise, the very simplest job to run NWChem in batch using Load
Leveller is something like this
\begin{verbatim}
#!/bin/csh -x
# @ job_type         =    parallel
# @ class            =    small
# @ requirements     =    (Adapter == "hps_user")
# @ input            =    /dev/null
# @ output           =    <OUTPUT_FILE_NAME>
# @ error            =    <ERROUT_FILE_NAME>
# @ environment      =    COPY_ALL; MP_EUILIB=us ; MP_CSS_INTERRUPT=yes
# @ min_processors   =    7
# @ max_processors   =    7
# @ cpu_limit        =    1:00:00
# @ queue
#

cd /scratch

nwchem <INPUT_FILE_NAME>
\end{verbatim}

Substitute \verb+<OUTPUT_FILE_NAME>+, \verb+<ERROUT_FILE_NAME>+ and
\verb+<INPUT_FILE_NAME>+ with the {\em full} path of the appropriate
files.  These files and the NWChem executable must be in a file system
accessible to all processes.  Put the above into a file (e.g.,
\verb+"test.job"+) and submit it with the command
\begin{verbatim}
  llsubmit test.job
\end{verbatim}
It will run a 7 processor, 1 hour job in the queue \verb+small+.  It
should be apparent how to change these values.

Note that on many IBM SPs, including that at EMSL, the local scratch
disks are wiped clean at the beginning of each job and therefore
persistent files should be stored elsewhere.  PIOFS is recommended for
files larger than a few MB.

\section{Cray T3E}

\begin{verbatim}
  mpprun -n n nwchem <input_file>
\end{verbatim}

where \verb+n+ is the number of processors and \verb+input_file+ is the
name of your input file.

\section{Linux}

If running in parallel across multiple machines you should consider
applying this patch to your kernel to boost the performance of TCP/IP
\begin{itemize}
\item \htmladdnormallink{http://www.icase.edu/coral/LinuxTCP.html}{http://www.icase.edu/coral/LinuxTCP.html}
\end{itemize}

\section{Tested Platforms and O/S versions}

\begin{itemize}
\item IBM SP with P2SC nodes,  AIX 4.2.1, and PSSP 2.3
\item IBM SP with silver nodes (SMP nodes with two 604e processors),
AIX 4.3.2, and PSSP 3.1.
\item IBM RS6000 workstation, AIX 3.2 and 4.1.
\item Cray T3E, 2.0.4.61 UNICOSMK
\item SGI R8000/10000, IRIX 6.2, 6.5
\item SGI R4000, IRIX 5.3
\item SUN workstations, SunOS 4.1.3 and Solaris 5.5, 5.6
\item Compaq DEC alpha workstion 
\begin{itemize}
\item (600 MHz EV6), Digital UNIX V4.0E Rev. 1091, DEC C V5.8-009, Digital Fortran V5.2
\item (667 MHz EV6), Digital UNIX V4.0F Rev. 1229, DEC C V5.9-005, Digital Fortran V5.2
\end{itemize}
\item Linux. Since there are at least 8 popular distributions of the
Linux operating system and numerous others in existence, including
downloading everything and building your own Linux OS, it is
impossible to test all possible versions of Linux with NWChem. NWChem
Release 3.3 has been tested on Slackware 3.4, 3.5, 4.0, 7.0 RedHat
5.1, 5.2, and 6.0, 6.1, 6.2 Mandrake based on RedHat 6.0, RedHat 6.0 for
the Power PC Macintosh, and Black Lab 1.2 or Yellow Dog 1.2 Linux for
Power PC Macintosh.  These all use the EGCS compilers at different
levels.  Those distributed from Slackware are somewhat different than
those distributed from RedHat but the code is configured to run on all
of them.  The Linux Alpha version 5.2 from Red Hat fails to compile
the code as well as the beta release of the Digital Fortran compiler
for Linux based Alpha systems.  
\item FreeBSD.  Just use the Linux target and NWChem will build for FreeBSD
version 4.0 and the pre-release 5.0 versions of the Operating System.

\end{itemize}
